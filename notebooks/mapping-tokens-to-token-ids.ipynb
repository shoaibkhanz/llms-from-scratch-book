{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0960b13d-d595-4001-bd3e-e8781fc2674b",
   "metadata": {},
   "source": [
    "# Mapping tokens to token IDs\n",
    "### Creating the first version of the simple tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6131e2f8-fe36-42bf-adef-5818ebd97a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b59178-6e77-4987-b233-198c23055b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the text 20559\n",
      "\n",
      " The Verdict: Edith Wharton: 1908\n",
      "Exported from Wikisource on October 21, 2024\n",
      "\n",
      "I HAD always thought\n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"../resources/verdict.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    verdict = f.read()\n",
    "\n",
    "print(f\"length of the text {len(verdict)}\")\n",
    "print(\"\\n\", verdict[:99])\n",
    "\n",
    "# We have now confirmed the length of the text, and the also printed the first 99 characters and \n",
    "# the length includes the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e7f136-fee2-4aa6-b633-980ef6b6b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex_logic = r\"([,.:;?_!\\\"()']|--|\\s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3b101b-d09b-4aac-906f-39320cf9b9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = re.split(regex_logic, verdict)\n",
    "preprocessed_text = [text.strip() for text in preprocessed_text if text.strip()]\n",
    "print(len(preprocessed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a724005-47ad-4487-8e0d-37e2bf29437c",
   "metadata": {},
   "source": [
    "Above we have simply copied the code as we step into further processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4b0579-cf3e-4dc7-af8a-972cd11dff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed_text))\n",
    "\n",
    "\n",
    "#word_mapping = {}\n",
    "#for i,word in enumerate(all_words):\n",
    "#    word_mapping[word]= i\n",
    "\n",
    "# much more concise way of writing the above, with a slight change in the output using () \n",
    "# would be the following\n",
    "vocab = {word: idx for idx, word in enumerate(all_words)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a567e0e2-c958-416e-957e-c6a4558f9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "('1908', 8)\n",
      "('2024', 9)\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(vocab.items()):\n",
    "    if i < 10:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad144239-d062-4e49-a9b0-b46a65727a62",
   "metadata": {},
   "source": [
    "Now we further need to understand that simply converting one way isnt what we need, \n",
    "We need to be able to convert tokens to token ids but also get back tokens from token ids.\n",
    "This way when we apply the model we are able to generate tokens and understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ef6fc1-b8ba-4c01-bc9e-16aa519ba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab # this instance of class expects we have a vocab already defined\n",
    "        self.int_to_str = {idx:text for text,idx in vocab.items()}\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        preprocessed_text = re.split(regex_logic, text)\n",
    "        preprocessed_text = [text.strip() for text in preprocessed_text if text.strip()]\n",
    "        mapping_idx = [self.str_to_int[token] for token in preprocessed_text]\n",
    "        return mapping_idx\n",
    "        \n",
    "\n",
    "    def decode_text(self, index):\n",
    "        joined_idx= \" \".join(self.int_to_str[idx] for idx in index)\n",
    "        joined_idx = re.sub(r\"\\s+([,?.!\\\"()'])\", r\"\\1\", joined_idx)\n",
    "        # the above regex attempts to remove the whitespace before the special characters,\n",
    "        # this is achieved by the capturing group which suggest remove space but keep the capture group\n",
    "        # characters.\n",
    "        return joined_idx\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e639c46-1179-4bf9-8091-c6e821e82120",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c641be7c-2085-41de-8028-8321265f0aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103, 593, 124, 5, 657, 324]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokeniser.encode_text(\"This is a ,lovely day\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d699b9d1-8b54-44bf-a14e-fa687981e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a, lovely day'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.decode_text(token_ids) # notice how the space id removed just before the ,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
