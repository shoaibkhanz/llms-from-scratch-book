{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611c95fa-3bbd-44b9-9a59-8d647ffb6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62670f55-b8a9-4faf-afdc-0112765cb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcf2e7f-85ab-4633-8a0b-e9cd1c81cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ae0696-7c23-4658-80aa-586087acada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5170\n"
     ]
    }
   ],
   "source": [
    "enc_token = tokeniser.encode(raw_text)\n",
    "print(len(enc_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734103ad-707f-4be9-944e-fcbaddd7dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_token[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63e060a-06fe-45c8-b1f2-b7d9ef0f6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 4643, 11600, 25]\n",
      "[4643, 11600, 25, 1717]\n"
     ]
    }
   ],
   "source": [
    "context_length = 4\n",
    "input_tokens = enc_sample[0:context_length]\n",
    "target_tokens = enc_sample[1:context_length+1]\n",
    "print(input_tokens)\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c3ca38-23cc-46e8-8e5c-fd369a55cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464] --> 4643\n",
      "[464, 4643] --> 11600\n",
      "[464, 4643, 11600] --> 25\n",
      "[464, 4643, 11600, 25] --> 1717\n"
     ]
    }
   ],
   "source": [
    "# note here the we cannot iterate from 0 because that would mean enc_sample[:0] returns as empty array \n",
    "# thus we need to increase the stop range by 1, so we can show the input of len 4  \n",
    "for i in range(1,context_length+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired= enc_sample[i]\n",
    "    print(context, \"-->\", desired)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def47a63-c016-407e-b6f3-5b879edab4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -->  Ver\n",
      "The Ver --> dict\n",
      "The Verdict --> :\n",
      "The Verdict: -->  Ed\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_length+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # note how context is not in a list while desired is and thats because context returns\n",
    "    # a list of token ids while the desired object simply returns a value.\n",
    "    print(tokeniser.decode(context), \"-->\", tokeniser.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeabf9e1-3923-4a76-ad22-8bbc68032180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e4ac7ec-9234-4275-88a8-3f0125056619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "6\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# before we jump into the next section lets revisit a for loop with steps\n",
    "for i in range(0, 11, 3):\n",
    "    print(i)\n",
    "\n",
    "# you will notice here, that at each iteration we start with the integer at the <start> in this case\n",
    "# its 0, then we count the number of <steps> after the start number, so for <steps> 3,\n",
    "# we count 123 and return the last index in this case its 3 and then 6 and so on. \n",
    "# this logic is important to know as we look at the dataset class we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd349c1-50db-4457-87df-dd4e12cf0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,text, tokeniser: tiktoken.Encoding, max_seq_length, stride):\n",
    "        # the initialisation needs input and target ids in order to save and get them later once we\n",
    "        # have built a logic to define them, we should be able to assign them.\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # the max_seq_length is subtracted from the total length of the text since we want to make sure\n",
    "        # we have the same length vector available at the end of the iteration.\n",
    "        # the stride is an important parameter here, in the absence of it we would iterate by 1, \n",
    "        # however, we need to ensure that strides are so that they dont overlap between input and target\n",
    "        # in cases that they do we understand that it having it equal to max_seq_length ensures \n",
    "        # that there is no overlap.\n",
    "        for i in range(0, len(token_ids) - max_seq_length, stride):\n",
    "            inputs = token_ids[i:i+max_seq_length]\n",
    "            targets = token_ids[i+1: i+max_seq_length+1]\n",
    "            self.input_ids.append(torch.tensor(inputs))\n",
    "            self.target_ids.append(torch.tensor(targets))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369c4918-9895-4d23-9569-0b505fe1c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, max_seq_length, stride ,batch_size, shuffle, drop_last, num_workers):\n",
    "    tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokeniser, max_seq_length, stride)\n",
    "    dataloader = DataLoader(dataset= dataset, \n",
    "                            batch_size= batch_size,\n",
    "                            shuffle= shuffle, \n",
    "                            drop_last= drop_last,\n",
    "                            num_workers= num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54f8218-ed01-4f32-a614-9774fea3f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now use the dataloader and see an example using the verdict.txt dataset\n",
    "\n",
    "dataloader = create_dataloader(raw_text, max_seq_length= 4, stride=1,batch_size=1, shuffle=False,\n",
    "                              drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c39590a-3062-4f02-b15d-1d12a408d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b8696be-c30f-449b-ac1d-16c763387b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x114d12e90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets convert it to an iterator so we can inspect the components of the loader\n",
    "# an object that implements and __iter__ method can be converted to an iterator\n",
    "iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7110794d-8392-4b98-b2c2-e3e044ec0d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  464,  4643, 11600,    25]]), tensor([[ 4643, 11600,    25,  1717]])] \n",
      "\n",
      "Second batch: [tensor([[ 4643, 11600,    25,  1717]]), tensor([[11600,    25,  1717,   342]])]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the first and second iterations of the DataLoader.\n",
    "# In the first batch, we get a list of integers corresponding to the input and target sequences.\n",
    "# When we retrieve the second batch, we see that it consists of the same data but slightly shifted, \n",
    "# as determined by the stride parameter.\n",
    "\n",
    "# Key observations:\n",
    "# 1. The total number of batches is determined by the dataset size, `max_seq_length`, and the stride.\n",
    "# 2. Each batch contains a fixed number of elements, defined by `max_seq_length` (in this case, 4).\n",
    "# 3. The stride of 1 means that consecutive batches have overlapping sequences, with each new batch \n",
    "#    shifted by 1 position compared to the previous batch.\n",
    "\n",
    "# Example:\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch, \"\\n\")\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b8ee96-e58f-4b37-ba57-cecf385515fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try stride 2 and 3\n",
    "stride2 = create_dataloader(raw_text, max_seq_length= 4, stride=2,batch_size=1, shuffle=False,\n",
    "                              drop_last=True, num_workers=0)\n",
    "\n",
    "stride3 = create_dataloader(raw_text, max_seq_length= 4, stride=3,batch_size=1, shuffle=False,\n",
    "                              drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8323ebcd-f2ce-4ce3-851b-99bedeac753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  4643, 11600,    25]]), tensor([[ 4643, 11600,    25,  1717]])]\n",
      "[tensor([[11600,    25,  1717,   342]]), tensor([[  25, 1717,  342,  854]])]\n"
     ]
    }
   ],
   "source": [
    "iter_stride2 = iter(stride2)\n",
    "print(next(iter_stride2))\n",
    "print(next(iter_stride2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b47912c-4685-439d-a47f-5e96455976a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  4643, 11600,    25]]), tensor([[ 4643, 11600,    25,  1717]])]\n",
      "[tensor([[  25, 1717,  342,  854]]), tensor([[ 1717,   342,   854, 41328]])]\n"
     ]
    }
   ],
   "source": [
    "iter_stride3 = iter(stride3)\n",
    "print(next(iter_stride3))\n",
    "print(next(iter_stride3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f6eb1ff-d867-4396-9505-e5c8a78719f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets also look at a larger batch size\n",
    "large_batch= create_dataloader(raw_text, max_seq_length= 4, stride=2,batch_size=4, shuffle=False,\n",
    "                              drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4cf65a7-b719-4b02-aa04-c604cdf40995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  4643, 11600,    25],\n",
      "        [11600,    25,  1717,   342],\n",
      "        [ 1717,   342,   854, 41328],\n",
      "        [  854, 41328,    25, 40417]]), tensor([[ 4643, 11600,    25,  1717],\n",
      "        [   25,  1717,   342,   854],\n",
      "        [  342,   854, 41328,    25],\n",
      "        [41328,    25, 40417,   198]])]\n"
     ]
    }
   ],
   "source": [
    "batch_iter = iter(large_batch)\n",
    "print(next(batch_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eda3599-f931-4e94-8fce-05974db6ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   25, 40417,   198,  3109],\n",
      "        [  198,  3109,  9213,   422],\n",
      "        [ 9213,   422, 11145,   271],\n",
      "        [11145,   271,  1668,   319]]), tensor([[40417,   198,  3109,  9213],\n",
      "        [ 3109,  9213,   422, 11145],\n",
      "        [  422, 11145,   271,  1668],\n",
      "        [  271,  1668,   319,  3267]])]\n"
     ]
    }
   ],
   "source": [
    "print(next(batch_iter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
