{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9fff52-ed86-444c-8753-2de875da0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6606c2b1-da28-4e44-a461-c01459b2bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your journey starts with one step\n",
    "torch.manual_seed(123)\n",
    "inputs = torch.rand((6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126aa32c-244e-4934-b19f-a6b7930d72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_2: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# First way\n",
    "# here x_2 is called the query and we do a dot operation\n",
    "x_2 = inputs[1] # query\n",
    "print(f\"shape of x_2: {x_2.shape}\")\n",
    "attention_scores1 = torch.empty(len(inputs))\n",
    "for i, inp in enumerate(inputs):\n",
    "    attention_scores1[i] = (x_2 @ inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7af8df8-c0c3-407b-a609-3a0d35116eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54811e7d-f593-4e24-ad16-b4387b450253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way\n",
    "# something that I learned is the dot method below, \n",
    "# Computes the dot product of two 1D tensors. and not 2D or more\n",
    "# for that we can use @ , as you can see below in cell 7\n",
    "x_2 = inputs[1]\n",
    "attention_scores2 = torch.empty(len(inputs))\n",
    "for i, inp in enumerate(inputs):\n",
    "    attention_scores2[i] = torch.dot(x_2,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba22d25-f939-4b6e-811a-1062229a22fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2169f5-ba48-494c-a7df-606d1b401713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023])\n"
     ]
    }
   ],
   "source": [
    "# Third way\n",
    "attention_scores3= inputs @ x_2\n",
    "print(attention_scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd06abe-2a73-4fad-976a-66f107da0542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2961, 0.5166, 0.2517])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221a1e94-26fd-4f0b-ba3a-a85f7202aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: tensor([0.2961, 0.5166, 0.2517])\n",
      "x_2: tensor([0.6886, 0.0740, 0.8665])\n",
      "tensor(0.4602)\n"
     ]
    }
   ],
   "source": [
    "# dot product between 1d tensor\n",
    "print(f\"inputs[0]: {inputs[0]}\")\n",
    "print(f\"x_2: {x_2}\")\n",
    "res = 0\n",
    "for idx, val in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * x_2[idx] # query\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f6d7e00-4b1f-4125-8776-84295b01574a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edaf0156-6eb4-4b47-97b3-8f2597983fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7c58d2-b4a3-4e1c-83dc-9ebff535348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs@ x_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c69fc99-8528-4323-b462-a21e748f07e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53b34cd-7063-48c4-94bb-103c777b9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores: tensor([0.1043, 0.2788, 0.0592, 0.2535, 0.0772, 0.2271])\n",
      "attention scores sum to one: 1.0\n"
     ]
    }
   ],
   "source": [
    "# normalisation of attention scores converts them to attention weights\n",
    "# at this point they sum to 1 and help in training stability\n",
    "attention_weights = attention_scores3 / attention_scores3.sum()\n",
    "print(f\"attention scores: {attention_weights}\")\n",
    "# to show that the tensor now sums to 1, we can sum the full tensor\n",
    "print(f\"attention scores sum to one: {attention_weights.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07e8ffa-b91f-4cdf-912d-5f74f9d526c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4179, 0.4602, 0.1397, 0.5509, 0.2036, 0.3884],\n",
       "        [0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023],\n",
       "        [0.1397, 0.2611, 0.0630, 0.2580, 0.0887, 0.2193],\n",
       "        [0.5509, 1.1189, 0.2580, 1.0992, 0.3343, 0.8977],\n",
       "        [0.2036, 0.3408, 0.0887, 0.3343, 0.1445, 0.3155],\n",
       "        [0.3884, 1.0023, 0.2193, 0.8977, 0.3155, 0.8600]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a full computation of attention scores\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ffc8c-332d-4b31-a27a-93ff67006d58",
   "metadata": {},
   "source": [
    "The following examination tries to understand why dim=0 or dim=1 resulted in the same attention weights. It was found that since we are calculating a matrix product of `inputs` and `inputs transpose` this results in a symmetrical matrix.\n",
    "Now when we further calculate why the symmetrical matrix generates the same attention weights no matter how we sum it. I found that its primarily dependant on the sum of each row or column, which in this case generates the same ouput vector, see cells 28,29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c47ff8e-a0f6-4f0e-b5a5-0f495d95a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2961, 0.5166, 0.2517])\n",
      "tensor([0.3122, 0.3892, 0.2986])\n"
     ]
    }
   ],
   "source": [
    "# manually calculating softmax\n",
    "print(inputs[0])\n",
    "print(torch.exp(inputs[0])/torch.exp(inputs[0]).sum(dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93bb59d-e2b6-42b6-849d-3d7ceac88f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x,dim=0):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "855af625-d0fb-4316-8688-91b56c3de3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights_naive_soft = softmax_naive(attention_scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2381ac24-3f13-4b6e-9be5-7c833e888aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1174, 0.2536, 0.0962, 0.2268, 0.1042, 0.2019])\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights_naive_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f1aaa5-0505-4781-9d24-9719d346d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1748, 0.1174, 0.1609, 0.1340, 0.1603, 0.1268],\n",
      "        [0.1824, 0.2536, 0.1817, 0.2365, 0.1838, 0.2342],\n",
      "        [0.1324, 0.0962, 0.1490, 0.1000, 0.1429, 0.1070],\n",
      "        [0.1997, 0.2268, 0.1811, 0.2319, 0.1827, 0.2110],\n",
      "        [0.1411, 0.1042, 0.1529, 0.1079, 0.1511, 0.1179],\n",
      "        [0.1697, 0.2019, 0.1743, 0.1896, 0.1793, 0.2032]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_naive_soft_inputs1 = softmax_naive(attn_scores,dim=1)\n",
    "print(attention_weights_naive_soft_inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6cd265a-3359-4bae-96b7-ad56f49df0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_naive_soft_inputs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5465d040-fb32-4e76-851d-e6cbdf3e1112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1748, 0.1824, 0.1324, 0.1997, 0.1411, 0.1697])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights_naive_soft_inputs1[:, 0])\n",
    "print(attention_weights_naive_soft_inputs1[:, 0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc827f9e-07f2-4ebf-85f2-8c5d1e0e7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights_naive_soft_inputs2 = softmax_naive(attn_scores,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a38e87b5-fdbf-4eda-9439-d7deda444e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1748, 0.1824, 0.1324, 0.1997, 0.1411, 0.1697])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights_naive_soft_inputs2[:, 0])\n",
    "print(attention_weights_naive_soft_inputs2[:, 0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f55875c7-723c-4f77-a5c6-b62430191718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1748, 0.1174, 0.1609, 0.1340, 0.1603, 0.1268],\n",
       "        [0.1824, 0.2536, 0.1817, 0.2365, 0.1838, 0.2342],\n",
       "        [0.1324, 0.0962, 0.1490, 0.1000, 0.1429, 0.1070],\n",
       "        [0.1997, 0.2268, 0.1811, 0.2319, 0.1827, 0.2110],\n",
       "        [0.1411, 0.1042, 0.1529, 0.1079, 0.1511, 0.1179],\n",
       "        [0.1697, 0.2019, 0.1743, 0.1896, 0.1793, 0.2032]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(attn_scores)/torch.exp(attn_scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80cdb584-4021-4905-8c8b-0ccec074b4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1748, 0.1174, 0.1609, 0.1340, 0.1603, 0.1268],\n",
       "        [0.1824, 0.2536, 0.1817, 0.2365, 0.1838, 0.2342],\n",
       "        [0.1324, 0.0962, 0.1490, 0.1000, 0.1429, 0.1070],\n",
       "        [0.1997, 0.2268, 0.1811, 0.2319, 0.1827, 0.2110],\n",
       "        [0.1411, 0.1042, 0.1529, 0.1079, 0.1511, 0.1179],\n",
       "        [0.1697, 0.2019, 0.1743, 0.1896, 0.1793, 0.2032]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(attn_scores)/torch.exp(attn_scores).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3ca210-31dc-4ecd-944b-30f9cee3cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4179, 0.4602, 0.1397, 0.5509, 0.2036, 0.3884],\n",
      "        [0.4602, 1.2304, 0.2611, 1.1189, 0.3408, 1.0023],\n",
      "        [0.1397, 0.2611, 0.0630, 0.2580, 0.0887, 0.2193],\n",
      "        [0.5509, 1.1189, 0.2580, 1.0992, 0.3343, 0.8977],\n",
      "        [0.2036, 0.3408, 0.0887, 0.3343, 0.1445, 0.3155],\n",
      "        [0.3884, 1.0023, 0.2193, 0.8977, 0.3155, 0.8600]])\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90efc614-d379-40b4-bf03-25affed94d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.6883, 13.4977,  7.1457, 12.9435,  7.6481, 11.6328])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(attn_scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d9d6fd-a7d5-44b6-88ff-2b65c52b839a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.6883, 13.4977,  7.1457, 12.9435,  7.6481, 11.6328])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.exp(attn_scores).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d925a-d823-4b9e-bd76-9bb83acfda59",
   "metadata": {},
   "source": [
    "In following 4 cells, I try to compute the attention weight for the first index, so as to understand the process at the more fundamental level, This builds the intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a21034db-1b01-49d0-9a75-f50bf17e2171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5187, 1.5844, 1.1499, 1.7348, 1.2258, 1.4747],\n",
       "        [1.5844, 3.4227, 1.2984, 3.0615, 1.4061, 2.7247],\n",
       "        [1.1499, 1.2984, 1.0651, 1.2943, 1.0928, 1.2452],\n",
       "        [1.7348, 3.0615, 1.2943, 3.0018, 1.3970, 2.4540],\n",
       "        [1.2258, 1.4061, 1.0928, 1.3970, 1.1555, 1.3709],\n",
       "        [1.4747, 2.7247, 1.2452, 2.4540, 1.3709, 2.3632]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc41acd-9397-4e89-aee9-d49284cc115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1748"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.5187/8.688,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad70f03-1561-444e-b9da-5a085ef4ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1824"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.5844/8.6883,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1447d082-ab54-4671-91ed-370d811ea370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1174"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.5844/13.4997,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b944b6-3d8c-45f6-8c07-fa6a00db4309",
   "metadata": {},
   "source": [
    "Now lets use softmax function available within PyTorch, this softmax function is numerically stable, with better implementation when it encounters low values and large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63d1e9ff-64c1-4441-b4f8-381128309a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1174, 0.2536, 0.0962, 0.2268, 0.1042, 0.2019])\n"
     ]
    }
   ],
   "source": [
    "attention_weights1 = torch.softmax(attention_scores3,dim=0)\n",
    "print(attention_weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ace258f8-5816-4e1d-a757-e2e57610e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1748, 0.1174, 0.1609, 0.1340, 0.1603, 0.1268],\n",
      "        [0.1824, 0.2536, 0.1817, 0.2365, 0.1838, 0.2342],\n",
      "        [0.1324, 0.0962, 0.1490, 0.1000, 0.1429, 0.1070],\n",
      "        [0.1997, 0.2268, 0.1811, 0.2319, 0.1827, 0.2110],\n",
      "        [0.1411, 0.1042, 0.1529, 0.1079, 0.1511, 0.1179],\n",
      "        [0.1697, 0.2019, 0.1743, 0.1896, 0.1793, 0.2032]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights2 = torch.softmax(attn_scores,dim=0)\n",
    "print(attention_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b0019-f537-4056-9255-fbf8c4a3fe02",
   "metadata": {},
   "source": [
    "Now to get the context vectors, we need to consider the inputs and attention weights calculated uptil now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2e66f74-f425-4fe6-b784-2c4f4478e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3150, 0.2043, 0.4230],\n",
       "        [0.5332, 0.2701, 0.7136],\n",
       "        [0.2522, 0.1631, 0.3466],\n",
       "        [0.5071, 0.2725, 0.6718],\n",
       "        [0.2716, 0.1740, 0.3734],\n",
       "        [0.4460, 0.2396, 0.6048]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(attention_weights2.shape)\n",
    "print(inputs.shape)\n",
    "attention_weights2 @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "427705f4-c411-4fe7-b7a9-55557ecde7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4762, 0.2052, 0.6228])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights1 @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe83bcab-959a-41bd-abfc-5e26c47c87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do it on a loop\n",
    "context_vec = torch.zeros(inputs[0].shape)\n",
    "for idx, x_i in enumerate(inputs):\n",
    "        context_vec += attention_weights1[idx] * x_i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "702b18d0-4ef6-4fc5-9b5f-d97a0416fb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4762, 0.2052, 0.6228])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb6669-8218-41e1-be41-bea97a641d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
