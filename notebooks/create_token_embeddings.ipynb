{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230790b3-1d5f-48a7-a54b-719265d19cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1929a4-3e0d-4f91-8f64-0d1fd36fec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "vocab_size= 6\n",
    "output_dim=3\n",
    "embeddings = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557f7ab0-1c37-44da-ba48-0800df6b6956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.sparse.Embedding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c184f7-0ad6-4f10-86fb-890272e772d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings are essentially a big lookup\n",
    "embeddings(torch.tensor([3])) # this returns the 4th row of the above embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472cca32-5f6b-418e-a92c-1d9be04abd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "print(embeddings(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca68d2b-145e-4907-a5f5-a63de94ac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going over a more realistic example\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embeddings_layer= torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f07e50f-f1eb-498f-a0a4-710fe478b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe11cab-3448-42f2-a43d-78c9867bbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, raw_text, tokeniser, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens = tokeniser.encode(raw_text)\n",
    "        for i in range(0, len(tokens) - max_length, stride):\n",
    "            inputs = tokens[i: i +max_length]\n",
    "            targets = tokens[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(inputs))\n",
    "            self.target_ids.append(torch.tensor(targets))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "def create_dataloader(raw_text, tokeniser, max_length, stride, \n",
    "                      batch_size, shuffle=False, drop_last=True):\n",
    "    dataset = GPTDataset(raw_text, tokeniser, max_length, stride)\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size, shuffle= shuffle, drop_last= drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98856d2-a69d-4a09-92e4-b2962342be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20559\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/verdict.txt\") as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91d8e85-0d35-47aa-bb4a-5b12009293ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "max_length = 4\n",
    "batch_size =8\n",
    "dataloader = create_dataloader(raw_text, tokeniser,max_length, \n",
    "                               stride= max_length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc3d861-4cfd-4204-902f-e50fb3310352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  4643, 11600,    25],\n",
      "        [ 1717,   342,   854, 41328],\n",
      "        [   25, 40417,   198,  3109],\n",
      "        [ 9213,   422, 11145,   271],\n",
      "        [ 1668,   319,  3267,  2310],\n",
      "        [   11, 48609,   198,   198],\n",
      "        [   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271]]), tensor([[ 4643, 11600,    25,  1717],\n",
      "        [  342,   854, 41328,    25],\n",
      "        [40417,   198,  3109,  9213],\n",
      "        [  422, 11145,   271,  1668],\n",
      "        [  319,  3267,  2310,    11],\n",
      "        [48609,   198,   198,    40],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader_iter= iter(dataloader)\n",
    "print(next(dataloader_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b760f7-6546-4ed7-a0de-3f422dc4abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a517814c-51ca-48c9-88ed-31c2e6652a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11],\n",
      "        [  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339]]) \n",
      "\n",
      "targets:  tensor([[ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287],\n",
      "        [  262,  6001,   286,   465],\n",
      "        [13476,    11,   339,   550]])\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs: \", inputs, \"\\n\")\n",
    "print(\"targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8d2082-612d-418a-9c4a-0bb5c8a92aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape:  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs shape: \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05cc870b-08b6-4874-be0e-da6e38ce95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa7d5d5-3dd8-4e63-a1ab-22bd7bb28a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41cb9e3-9527-40f4-afd3-3c43bb1d34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embeddings_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1c3c75-5fd5-4f78-90cd-06b5c59eae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14357c62-05e7-4f64-ae55-e57e56da4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541d70da-d988-4863-979f-0024ed899785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just to test if I can add the axis and get same result\n",
    "pos_embedding_dim_correction = pos_embedding.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f906b25-e569-4d89-8035-d609873ac561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(pos_embedding_dim_correction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57cea774-54d1-47e1-a9df-a3d2ab709a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embedding_dim_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "362d8890-5486-487b-8ca8-a6fae39a6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
