{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aac8b58-33fc-4b66-b893-85e23ec3cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de07a44-822a-4015-b933-63ec6b184c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath: Path):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "    return raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd0d517-f3df-48ad-b264-df741b264061",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../resources/verdict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9ac4ae-286d-46f1-a79a-a03ffc1cd09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20559\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6d2b76-a5a9-4f27-9127-85095fa899d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = re.split(r'([,.?_!\"()\\']|--|\\s)', data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97dc4268-3a31-46e5-9dc9-f8cf84ad06ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', ' ', 'Verdict:', ' ', 'Edith', ' ', 'Wharton:', ' ', '1908', '\\n', 'Exported', ' ', 'from', ' ', 'Wikisource', ' ', 'on', ' ', 'October', ' ', '21', ',', '', ' ', '2024', '\\n', '', '\\n', 'I', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbddb618-3af6-44b0-a5f0-27ea5eb80cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens= sorted(list(set(preprocessed_data)))\n",
    "vocab_size = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00dabaa9-51dc-4253-aabe-2bf649f09b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=1171\n"
     ]
    }
   ],
   "source": [
    "print(f\"{vocab_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1a0f33-22ab-45d9-9983-c33ff818dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:idx for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d37fb41c-2dbc-4064-8c1c-536ce7864c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 0), ('\\n', 1), (' ', 2), ('!', 3), ('\"', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cc5a39-e9d9-4831-bccb-912c4fa72a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokeniser:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab #vocab is already processed and passed to the class.\n",
    "        self.int_to_str = {int:str for str,int in vocab.items()}\n",
    "\n",
    "\n",
    "    def encode(self,text, pattern = r'([,.?_!\"()\\']|--|\\s)'):\n",
    "        preprocessed_data = re.split(pattern,text)\n",
    "        #here the condition makes sure that no whitespaces are passed for e.g. \"\\n\" or \" \"\n",
    "        preprocessed_data = [item.strip() for item in preprocessed_data if item.strip()] \n",
    "        ids = [self.str_to_int[string] for string in preprocessed_data]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text= \" \".join([self.int_to_str[idx] for idx in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10680cf4-d6c8-4282-8866-69486b37de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SimpleTokeniser(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8468f26-1896-4066-b638-97bd80c31d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 66, 5, 884, 1025, 627, 553, 775, 8, 1167, 620, 8, 4, 77, 10, 47, 885, 1148, 785, 824, 10]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "idx = tokeniser.encode(text)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55e57a4d-5788-4c2d-a8e5-19396d0b1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "text = tokeniser.decode(idx)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701d5fb-d8a1-44b2-8088-ce00fb5da456",
   "metadata": {},
   "source": [
    "adding `<|unk|>` and `<|endoftext|>` to the represent unknown words and to seperate 2 unrelated content respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a6caa1-febb-4c6b-a0c4-1a6f3319a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(set(preprocessed_data))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token:idx for idx, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a3ca80-4e28-464d-b459-222696191c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n"
     ]
    }
   ],
   "source": [
    "# plese note that previsouly when we created the vocab its length was 1171 and now its 1173 as we added 2 tokens.\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de765d79-dba5-43e6-bf9d-8f08b3c8f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('younger', 1168), ('your', 1169), ('yourself', 1170), ('<|endoftext|>', 1171), ('<|unk|>', 1172)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.items())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41dcf39d-e4a2-4fd7-b1f9-615eb04d3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokeniserV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab #vocab is already processed and passed to the class.\n",
    "        self.int_to_str = {int:str for str,int in vocab.items()}\n",
    "\n",
    "\n",
    "    def encode(self,text, pattern = r'([,.?_!\"()\\']|--|\\s)'):\n",
    "        preprocessed_data = re.split(pattern,text)\n",
    "        #here the condition makes sure that no whitespaces are passed for e.g. \"\\n\" or \" \"\n",
    "        preprocessed_data = [item.strip() for item in preprocessed_data if item.strip()] \n",
    "        preprocessed_data = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed_data]\n",
    "        ids = [self.str_to_int[string] for string in preprocessed_data]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text= \" \".join([self.int_to_str[idx] for idx in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ebfd445-c039-46f5-b392-dcbecb216759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SimpleTokeniserV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e24b86a0-2c6c-470c-91fa-9f7c01d64b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace\"\n",
    "text = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7a9b694-95d1-4eff-b2d7-fd4db188f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1172, 8, 374, 1167, 654, 1012, 16, 1171, 65, 1025, 993, 1021, 750, 1025, 1172]\n"
     ]
    }
   ],
   "source": [
    "ids = tokeniser.encode(text) #1171 represents <|endoftext> and 1172 <|unk|> since hello was not part of the vocab\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f14e698-de32-4c5a-822c-a244f2dbf926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db8683-e414-4ce3-9f6c-a750785e2bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
