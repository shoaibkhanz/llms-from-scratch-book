{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c08d74-1eff-4917-9bb5-7683cf515824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6959e3b6-77fe-4a73-9589-fc15020f0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = Path().cwd().parents[0]\n",
    "sys.path.append(str(cur_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28520ea8-9ac3-4593-8eb8-3ec29e27381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.transformer_block import TransformerBlock, LayerNorm\n",
    "from src.model.train_model import GPTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea09c9e-ef90-4407-b543-563de836bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel_v2(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_layer = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.result_layer = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we get a bactch of data with a given sequence length\n",
    "        batch_size, seq_len = x.shape\n",
    "        token_embeddings = self.embedding_layer(x)\n",
    "        pos_embeddings = self.positional_layer(torch.arange(seq_len, device=x.device))\n",
    "        input_embeddings = pos_embeddings + token_embeddings\n",
    "        dropout = self.dropout(input_embeddings)\n",
    "        trans_block = self.transformer_block(dropout)\n",
    "        final_norm = self.final_norm(trans_block)\n",
    "        logits = self.result_layer(final_norm)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97a648a-f94a-4604-9e07-4832e9aff1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_small_config = {\n",
    "    \"vocab_size\": 50257,  # Size of the vocabulary used by the model\n",
    "    \"context_length\": 1024,  # Maximum length of input sequences\n",
    "    \"emb_dim\": 256,  # Dimensionality of the model's embeddings (d_model)\n",
    "    \"n_heads\": 16,  # Number of attention heads in the multi-head attention mechanism\n",
    "    \"n_layers\": 24,  # Number of transformer layers in the model\n",
    "    \"drop_rate\": 0.1,  # Dropout rate for regularization\n",
    "    \"qkv_bias\": False,  # Whether to include bias terms in the query, key, and value projections\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel_v2(GPT2_small_config)\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb212445-96f2-43e6-8a4a-37463842a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, max_seq_length,stride, batch_size, tokenizer,drop_last):\n",
    "    dataset = GPTDataset(text,tokenizer= tokenizer,max_seq_length=max_seq_length, stride=stride)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,drop_last=drop_last)  \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7c739-f8c3-42fe-84fa-30677e3a579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,weight_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddae035a-9107-47be-9b66-89ec70ae6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09ece6d-147f-45f2-809b-e88d57f9e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, device, \n",
    "                    num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \"\"\"\n",
    "    model: gpt model for next token prediction\n",
    "    train_dataloder: loading the training datav\n",
    "    val_dataloader: loading the validation data for evaluation\n",
    "    optimizer: adamW optimizer with a learning rate and weight decay\n",
    "    device: use cuda if available otherwise use cpu \n",
    "    num_epochs: number of training rounds\n",
    "    eval_freq: determines how frequently we evaluate and print the model performance\n",
    "    eval_iter: this is passed to the evaluation strategy\n",
    "    start_context: this is passed to the next token generation strategy, this essentially directs \n",
    "                    the model to generate text after x tokens before the last one\n",
    "    tokenizer: this is a gpt tokenizer, that tokenises the text into tokens with a gpt2 vocabulary.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
    "    tokens_seen , global_step = 0,-1\n",
    "    for e in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_dataloader, \n",
    "                                                      device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epochs {e+1} (Step{global_step:06d}): \"\n",
    "                      f\"Train loss: {train_loss:.3f}, \"\n",
    "                    f\"Val loss: {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869bfe1-047c-43c1-8bf2-99a6e159913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    inputs_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    torch.nn.functional.cross_entropy(logits.flatten())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b09cc-6f55-47b9-b356-5eb27795070e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
